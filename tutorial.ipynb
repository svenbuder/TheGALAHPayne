{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** This file gives a brief overview of the capabilities of the code. **\n",
    "\n",
    "* If you want to predict the spectrum of a star with particular labels, you'll want the \"spectral_model\" package.\n",
    "* If you want to fit an observed spectrum, see the \"fitting\" package.\n",
    "* Downloading and processing APOGEE spectra is handled by the \"process_spectra\" package.\n",
    "* The \"utils\" package contains some general-purpose functions used by the other packages.\n",
    "* If you want to get under the hood and train your own models, there some functions in the train_NNs/ directory to get you started.\n",
    "\n",
    "The model interpolator requires you to pass it the trained neural network (really, a list of biases and weights parameterizing the network), so we read in the network we'll be using at the beginning and then pass it to various functions as we go. This is a bit cumbersome, but the advantage is that if you train a new network (with architechture compatible with the existing code) you can just pass it to the relevant functions without having to rewrite everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function # Python2 compatibility\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from The_Payne import utils\n",
    "from The_Payne import spectral_model\n",
    "from The_Payne import fitting\n",
    "\n",
    "# the following will be used throughout all routines\n",
    "# these are the default for fitting APOGEE spectra \n",
    "# substitutes them if you train different neural networks for other purposes\n",
    "# the instruction for training a new Payne is included at the end of this tutorial\n",
    "\n",
    "# read in the default wavelength array, \n",
    "#         the apogee mask/filter used for fitting spectra : True = excluded\n",
    "wavelength = utils.load_wavelength_array()\n",
    "mask = utils.load_apogee_mask()\n",
    "#mask = np.zeros(wavelength.size) # no masking\n",
    "\n",
    "# read in the default neural networks\n",
    "NN_coeffs = utils.read_in_neural_network()\n",
    "w_array_0, w_array_1, w_array_2, b_array_0, b_array_1, b_array_2, x_min, x_max = NN_coeffs\n",
    "\n",
    "# if you trained your own neural networks (see last part of this tutorial),\n",
    "# you can load in your own neural networks\n",
    "#tmp = np.load(\"NN_normalized_spectra.npz\")\n",
    "#w_array_0 = tmp[\"w_array_0\"]\n",
    "#w_array_1 = tmp[\"w_array_1\"]\n",
    "#w_array_2 = tmp[\"w_array_2\"]\n",
    "#b_array_0 = tmp[\"b_array_0\"]\n",
    "#b_array_1 = tmp[\"b_array_1\"]\n",
    "#b_array_2 = tmp[\"b_array_2\"]\n",
    "#x_min = tmp[\"x_min\"]\n",
    "#x_max = tmp[\"x_max\"]\n",
    "#tmp.close()\n",
    "#NN_coeffs = (w_array_0, w_array_1, w_array_2, b_array_0, b_array_1, b_array_2, x_min, x_max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the data-driven spectral model to predict the APOGEE-like spectrum of a single star similar to the Sun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_err = 1e-2*np.ones(len(wavelength))\n",
    "\n",
    "# for a single-star model, the format of \"labels\" is [Teff, Logg, Vturb [km/s],\n",
    "#              [C/H], [N/H], [O/H], [Na/H], [Mg/H],\\\n",
    "#              [Al/H], [Si/H], [P/H], [S/H], [K/H],\\\n",
    "#              [Ca/H], [Ti/H], [V/H], [Cr/H], [Mn/H],\\\n",
    "#              [Fe/H], [Co/H], [Ni/H], [Cu/H], [Ge/H],\\\n",
    "#              C12/C13, Vmacro [km/s], radial velocity (RV)\n",
    "real_labels = scaled_labels = [5770, 4.44, 1.0,\\\n",
    "                               0., 0., 0., 0., 0.,\\\n",
    "                               0., 0., 0., 0., 0.,\\\n",
    "                               0., 0., 0., 0., 0.,\\\n",
    "                               0., 0., 0., 0., 0.,\\\n",
    "                               90., 6., 3.] # assuming RV = 3 km/s. \n",
    "\n",
    "# scale the labels (except for RV) the same as it was done during the training of the network\n",
    "scaled_labels[:-1] = (real_labels[:-1]-x_min)/(x_max-x_min) - 0.5\n",
    "print(np.array(scaled_labels).shape)\n",
    "\n",
    "real_spec = spectral_model.get_spectrum_from_neural_net(scaled_labels = scaled_labels[:-1], NN_coeffs = NN_coeffs)\n",
    "real_spec = utils.doppler_shift(wavelength, real_spec, scaled_labels[-1])\n",
    "\n",
    "# zoom in on a small region of the spectrum so we can see what's going on.\n",
    "lambda_min, lambda_max = 16000, 16100# for plotting \n",
    "m = (wavelength < lambda_max) & (wavelength > lambda_min)\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(wavelength[m], real_spec[m], 'k', lw=0.5)\n",
    "plt.xlim(lambda_min, lambda_max)\n",
    "plt.ylim(0.7, 1.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add some noise to this model spectrum, and then fit it to see if we can recover the labels we put in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spec = real_spec + 0.01*np.random.randn(len(real_spec))\n",
    "\n",
    "tol = 5e-4 # tolerance for when the optimizer should stop optimizing.\n",
    "\n",
    "# assuming your NN has two hidden layers. \n",
    "w_array_0, w_array_1, w_array_2, b_array_0, b_array_1, b_array_2, x_min, x_max = NN_coeffs\n",
    "    \n",
    "def fit_func(dummy_variable, *labels):\n",
    "    norm_spec = spectral_model.get_spectrum_from_neural_net(scaled_labels = labels[:-1], \n",
    "            NN_coeffs = NN_coeffs)\n",
    "    norm_spec = utils.doppler_shift(wavelength, norm_spec, labels[-1])\n",
    "    return norm_spec\n",
    "    \n",
    "# if no initial guess is supplied\n",
    "# here we operate in the scaled label space\n",
    "p0 = np.zeros(26)\n",
    "        \n",
    "# don't allow the minimimizer to go outside the range of training set\n",
    "bounds = np.zeros((2,26))\n",
    "bounds[0,:] = -0.5\n",
    "bounds[1,:] = 0.5\n",
    "bounds[0,-1] = -5.\n",
    "bounds[1,-1] = 5.\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "popt, pcov = curve_fit(fit_func, xdata=[], ydata = data_spec, sigma = spec_err, p0 = p0,\n",
    "                bounds = bounds, ftol = tol, xtol = tol, absolute_sigma = True, method = 'trf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spec = real_spec + 0.01*np.random.randn(len(real_spec))\n",
    "\n",
    "popt, pcov, model_spec = fitting.fit_normalized_spectrum_single_star_model(\\\n",
    "                                norm_spec = data_spec, spec_err = spec_err,\\\n",
    "                                NN_coeffs = NN_coeffs, wavelength = wavelength, mask=mask, p0 = None)\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "m = (wavelength < lambda_max) & (wavelength > lambda_min)\n",
    "plt.plot(wavelength[m], data_spec[m], 'k', lw=0.5, label = '\"data\" spec')\n",
    "plt.plot(wavelength[m], model_spec[m], 'r--', lw=0.5, label = 'best-fit model')\n",
    "plt.xlim(lambda_min, lambda_max)\n",
    "plt.legend(loc = 'best', frameon = False, fontsize = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that our best-fit labels are close to what we put in. \n",
    "print(popt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen how to generate and fit model spectra, let's download an actual APOGEE spectrum. Here we'll download a \"combined\" spectrum. \n",
    "\n",
    "Note: Downloading the spectra requires you to have Jo Bovy's Apogee package installed.\n",
    "\n",
    "Note: Here we adopt APOGEE DR14. Edit os.environs in the \"process_spectra\" source codes for a later version of APOGEE data release. Since our neural network training set was normalized using the DR12 wavelength definition, even thought the spectra are from DR14, we will resample them into the DR12 wavelength format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from The_Payne import process_spectra\n",
    "\n",
    "apogee_id = '2M18513961+4338099' # make sure the apogee id is in the right string format\n",
    "spec, spec_err = process_spectra.get_combined_spectrum_single_object(apogee_id = apogee_id, \n",
    "                    catalog = None, save_local = False)\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "m = (spec_err < 0.1) & (wavelength < lambda_max) & (wavelength > lambda_min)\n",
    "plt.plot(wavelength[m], spec[m], 'k', lw=0.5)\n",
    "plt.ylim(0.75, 1.05)\n",
    "plt.xlim(lambda_min, lambda_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit this spectrum with The-Payne-interpolated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popt, pcov, best_fit_spec = fitting.fit_normalized_spectrum_single_star_model(norm_spec = spec, \n",
    "        spec_err = spec_err, NN_coeffs = NN_coeffs, wavelength=wavelength, mask=mask, p0 = None)\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(wavelength[m], spec[m], 'k', lw=0.5, label = 'APOGEE spectrum')\n",
    "plt.plot(wavelength[m], best_fit_spec[m], 'r', lw=0.5, label = 'Best-fit model')\n",
    "plt.xlim(lambda_min, lambda_max)\n",
    "plt.ylim(0.7, 1.1)\n",
    "plt.legend(loc = 'best', frameon = False, fontsize= 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a different training grid, you can also train your own neural networks. But do remember to adopt, during spectral fitting, a different set of continuum pixels and/or spectroscopic mask tailored for your need. \n",
    "\n",
    "Note that, this part of the codes requires GPU (CUDA). It will not run if you don't have CUDA installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from The_Payne import training\n",
    "\n",
    "# load the default training set. Note that this training set is a small subset of what I used to train my network\n",
    "# due to the GitHub size limit, I only upload a subset of my training set, just for illustration\n",
    "training_labels, training_spectra, validation_labels, validation_spectra = utils.load_training_data()\n",
    "# label array = [n_spectra, n_labels]\n",
    "# spectra_array = [n_spectra, n_pixels]\n",
    "\n",
    "# The validation set is used to independently evaluate how well the neural networks\n",
    "# are emulating the spectra. If the networks overfit the spectral variation, while \n",
    "# the loss function will continue to improve for the training set, but the validation \n",
    "# set should show a worsen loss function.\n",
    "\n",
    "# the codes outputs a numpy saved array \"\"NN_normalized_spectra.npz\" \n",
    "# which can be imported and substitute the default neural networks (see the first part of this tutorial)\n",
    "training_loss, validation_loss = training.neural_net(training_labels, training_spectra,\\\n",
    "                                                     validation_labels, validation_spectra,\\\n",
    "                                                     num_neurons = 300, num_steps=1e5, learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss function of the training set and the validation set. The loss function has plateaued after around 1e4 steps. Here we were being conservative and trained 10 times longer. Depending on the scale of your problem, you might want to change the parameter \"num_steps\" above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.load(\"NN_normalized_spectra.npz\") # the output array also stores the training and validation loss\n",
    "training_loss = tmp[\"training_loss\"]\n",
    "validation_loss = tmp[\"validation_loss\"]\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(np.arange(training_loss.size)*1000, training_loss, 'k', lw=0.5, label = 'Training set')\n",
    "plt.plot(np.arange(training_loss.size)*1000, validation_loss, 'r', lw=0.5, label = 'Validation set')\n",
    "plt.legend(loc = 'best', frameon = False, fontsize= 18)\n",
    "plt.xlabel(\"Step\", size=20)\n",
    "plt.ylabel(\"Loss\", size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One practical note:**\n",
    "\n",
    "Fitting combined spectra with The Payne is pretty fast. If you pass the fitting function to a Python multiprocessing Pool, you should be able to comfortably fit 10,000 targets in < 1 day on a single node of a typical cluster. \n",
    "\n",
    "Training neural networks should not be too slow either if you have any decent GPU (even a cheap GTX 1060 should do). Training with any spectroscopic grid with 1000-10000 spectra should at most take a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
